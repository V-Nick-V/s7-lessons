{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382dd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755113b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la /lessons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24577c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb94a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ef0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls /user/data/master/events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -head /user/master/data/geo/events/date=2022-05-30/part-00013-82f846d5-74a4-44e0-8fa9-de643f825932.c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96180124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/nickperegr/prj7\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/nickperegr/prj7/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat /user/master/data/events/date=2022-05-31/part-00050-e1fe6a42-638b-4ad4-adc9-c7d0d312eef3.c000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -head /user/master/data/events/date=2022-06-08/part-00084-e1fe6a42-638b-4ad4-adc9-c7d0d312eef3.c000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/nickperegr/data/events/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bdc2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put -f geo.csv /user/nickperegr/data/cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7246b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/31 02:12:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"My first session\") \\\n",
    "        .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3192d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"My second session\") \\\n",
    "        .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), nullable=True),\n",
    "    StructField(\"Age\", LongType(), nullable=True),])\n",
    "\n",
    "data = [('Max', 55),\n",
    "        ('Yan', 53),\n",
    "        ('Dmitry', 54),\n",
    "        ('Ann', 25),]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema) \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bff3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .appName(\"Learning DataFrames\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.json(\"/user/master/data/events/date=2022-06-21/*.json\")\n",
    "\n",
    "events.show(10, False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af391e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Learning DataFrames\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.json(\"/user/master/data/events/\")\n",
    "\n",
    "events.show(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .appName(\"Sorting channel Type\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"/user/master/data/snapshots/channels/actual/*.parquet\")\n",
    "\n",
    "df.show(10);\n",
    "\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"channel_type\").save('/user/nickperegr/analytics/test')\n",
    "\n",
    "df1 = spark.read.parquet(\"/user/nickperegr/analytics/test/\")\n",
    "\n",
    "df1.select(\"channel_type\").orderBy(desc(\"channel_type\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .getOrCreate()\n",
    "# данные первого датафрейма \n",
    "book = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "        ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "        ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "        ('Catch-22', 'Joseph Heller',  174),\n",
    "        ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "        ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "        ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "        ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "]\n",
    "# данные второго датафрейма\n",
    "library = [\n",
    "        ( 322, \"1\"),\n",
    "        ( 250, \"2\" ),\n",
    "        (400, \"2\"),\n",
    "        (159, \"1\"),\n",
    "        (382, \"2\"),\n",
    "        (322, \"1\")\n",
    "]\n",
    "# названия атрибутов\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), nullable=True),\n",
    "    StructField(\"author\", StringType(), nullable=True),\n",
    "    StructField(\"book_id\", LongType(), nullable=True),])\n",
    "\n",
    "columns_library = ['book_id', 'Library_id']\n",
    "# создаём датафреймы\n",
    "df = spark.createDataFrame(data=book, schema=schema)\n",
    "df_library  = spark.createDataFrame(data=library, schema=columns_library )\n",
    "\n",
    "df_joined = df.join(df_library, [\"book_id\"], how = \"left_anti\").select(df.title)\n",
    "\n",
    "df_joined.show()\n",
    "\n",
    "df_joined_1 = df.join(df_library.filter(\"Library_id == '1'\"), [\"book_id\"], how = \"left_anti\")\n",
    "df_joined_2 = df.join(df_library.filter(\"Library_id == '2'\"), [\"book_id\"], how = \"left_anti\")\n",
    "\n",
    "df_joined_1.union(df_joined_2).show()\n",
    "\n",
    "df_joined_3 = df.join(df_library, [\"book_id\"], how = \"left\").collect()\n",
    "\n",
    "df_joined_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .getOrCreate()\n",
    "# данные первого датафрейма \n",
    "book_1 = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "        ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "        ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "        ('Catch-22', 'Joseph Heller',  174),\n",
    "        ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "        ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "        ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "        ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "]\n",
    "# данные второго датафрейма\n",
    "book_2 = [\n",
    "        ('Black Beauty',657 ,'Anna Sewell'),\n",
    "        ('Artemis Fowl',558,'Eoin Colfer'),\n",
    "        ('The Magic Faraway Tree', 567,'Enid Blyton'),\n",
    "        ('The Witches', 567,'Roald Dahl'),\n",
    "        ('Frankenstein',567 ,'Mary Shelley'),\n",
    "        ('The Little Prince',557 ,'Antoine de Saint-Exupéry'),\n",
    "        ('The Truth', 576 ,'Terry Pratchett')\n",
    "]\n",
    "# названия атрибутов\n",
    "columns_1= ['title', 'author', 'book_id']\n",
    "columns_2 = ['title', 'book_id', 'author']\n",
    "# создаём датафреймы\n",
    "df_1 = spark.createDataFrame(data=book_1 , schema=columns_1)\n",
    "df_2  = spark.createDataFrame(data=book_2 , schema=columns_2)\n",
    "# напишите ваш код ниже\n",
    "df = df_1.unionByName(df_2)\n",
    "\n",
    "df.show(truncate=0)\n",
    "df.show(df.count(),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ddee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Cache\") \\\n",
    "                    .getOrCreate()\n",
    "# данные первого датафрейма\n",
    "book = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "        ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "        ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "        ('Catch-22', 'Joseph Heller',  174),\n",
    "        ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "        ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "        ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "        ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "]\n",
    "# данные второго датафрейма\n",
    "library = [\n",
    "        ( 322, \"1\"),\n",
    "        ( 250, \"2\" ),\n",
    "        (400, \"2\"),\n",
    "        (159, \"1\"),\n",
    "        (382, \"2\"),\n",
    "        (322, \"1\")\n",
    "]\n",
    "# названия атрибутов\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), nullable=True),\n",
    "    StructField(\"author\", StringType(), nullable=True),\n",
    "    StructField(\"book_id\", LongType(), nullable=True),])\n",
    "\n",
    "columns_library = ['book_id', 'Library_id']\n",
    "# создаём датафреймы\n",
    "df = spark.createDataFrame(data=book, schema=schema)\n",
    "df_library  = spark.createDataFrame(data=library, schema=columns_library )\n",
    "\n",
    "df_joined = df.join(df_library, [\"book_id\"], how = \"left_anti\").select(df.title)\n",
    "\n",
    "df_joined.cache()\n",
    "\n",
    "df_joined.show()\n",
    "\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de96f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf())\n",
    "sc.setCheckpointDir(dirName=\"/user/nickperegr/content\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"CheckPoint\") \\\n",
    "                    .getOrCreate()\n",
    "# данные первого датафрейма\n",
    "book = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "        ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "        ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "        ('Catch-22', 'Joseph Heller',  174),\n",
    "        ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "        ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "        ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "        ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "]\n",
    "# данные второго датафрейма\n",
    "library = [\n",
    "        ( 322, \"1\"),\n",
    "        ( 250, \"2\" ),\n",
    "        (400, \"2\"),\n",
    "        (159, \"1\"),\n",
    "        (382, \"2\"),\n",
    "        (322, \"1\")\n",
    "]\n",
    "# названия атрибутов\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), nullable=True),\n",
    "    StructField(\"author\", StringType(), nullable=True),\n",
    "    StructField(\"book_id\", LongType(), nullable=True),])\n",
    "\n",
    "columns_library = ['book_id', 'Library_id']\n",
    "# создаём датафреймы\n",
    "df = spark.createDataFrame(data=book, schema=schema)\n",
    "df_library  = spark.createDataFrame(data=library, schema=columns_library )\n",
    "\n",
    "df_joined = df.join(df_library, [\"book_id\"], how = \"left_anti\").select(df.title)\n",
    "\n",
    "df_joined.checkpoint()\n",
    "\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93349f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .appName(\"func\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events_curr_day = spark.read.json(\"/user/master/data/events/\")\n",
    "#events= spark.read.json(\"/user/master/data/events/date=2022-06-21/*.json\")\n",
    "#events.show(10)\n",
    "#events_curr_day = events.withColumn(\"date\", F.current_date())\n",
    "events_curr_day.show(10)\n",
    "\n",
    "events1 = events_curr_day.withColumn(\"current_hour\", F.hour(events_curr_day.datetime))\n",
    "events2 = events1.withColumn(\"current_minute\", F.minute(events1.datetime))\n",
    "events3 = events2.withColumn(\"current_second\", F.second(events2.datetime))\n",
    "\n",
    "events4 = events3.orderBy(F.desc(\"datetime\"))\n",
    "\n",
    "events4.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .appName(\"func\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.json(\"/user/master/data/events/\")\n",
    "events.filter(F.col('event.message_to').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .appName(\"func\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.json(\"/user/master/data/events/\")\n",
    "events_type = events.select(events.event_type).distinct()\n",
    "events_type.show(events_type.count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .appName(\"reaction\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.json(\"/user/master/data/events/\")\n",
    "\n",
    "events_f = events.filter(F.col(\"event_type\")==\"reaction\") \\\n",
    "       .filter(F.col(\"event.datetime\").isNotNull()) \\\n",
    "       .groupBy(F.to_date(F.col(\"event.datetime\"))).count()\n",
    "\n",
    "events_f.orderBy(F.desc('count')).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"yarn\") \\\n",
    "                    .appName(\"Window\") \\\n",
    "                    .getOrCreate()\n",
    "# данные  датафрейма \n",
    "data = [('2021-01-06', 3744, 63, 322),\n",
    "        ('2021-01-04', 2434, 21, 382),\n",
    "        ('2021-01-04', 2434, 32, 159),\n",
    "        ('2021-01-05', 3744, 32, 159),\n",
    "        ('2021-01-06', 4342, 32, 159),\n",
    "        ('2021-01-05', 4342, 12, 259),\n",
    "        ('2021-01-06', 5677, 12, 259),\n",
    "        ('2021-01-04', 5677, 23, 499)\n",
    "]\n",
    "# названия атрибутов\n",
    "columns = ['dt', 'user_id', 'product_id', 'purchase_amount']\n",
    "# создаём датафрейм\n",
    "df = spark.createDataFrame(data=data, schema=columns) \n",
    "\n",
    "#window = Window().partitionBy(['purchase_amount']).orderBy(F.asc('purchase_amount'))\n",
    "window = Window().orderBy(F.asc('purchase_amount'), F.asc('user_id'))\n",
    "\n",
    "df_window = df.withColumn(\"row_number\", F.row_number().over(window))\n",
    "\n",
    "df_window.select(\"dt\", \"user_id\", \"purchase_amount\", \"row_number\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989081cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window  import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"yarn\") \\\n",
    "                    .appName(\"lag\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "events = spark.read.json(\"/user/master/data/events/\")\n",
    "\n",
    "window = Window().partitionBy('event.message_from').orderBy(F.col('date'))\n",
    "\n",
    "dfWithLag = events.withColumn(\"lag_7\",F.lag(\"event.message_to\", 7).over(window))\n",
    "\n",
    "dfWithLag.select(\"event.message_from\", \"date\", \"lag_7\") \\\n",
    ".filter(dfWithLag.lag_7.isNotNull()) \\\n",
    ".orderBy(F.desc(\"date\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3039032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .config(\"spark.driver.allowMultipleContexts\", \"true\") \\\n",
    "                    .appName(\"min-max\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [('2021-01-06', 3744, 63, 322),\n",
    "        ('2021-01-04', 2434, 21, 382),\n",
    "        ('2021-01-04', 2434, 32, 159),\n",
    "        ('2021-01-05', 3744, 32, 159),\n",
    "        ('2021-01-06', 4342, 32, 159),\n",
    "        ('2021-01-05', 4342, 12, 259),\n",
    "        ('2021-01-06', 5677, 12, 259),\n",
    "        ('2021-01-04', 5677, 23, 499)\n",
    "]\n",
    "\n",
    "columns = ['dt', 'user_id', 'product_id', 'purchase_amount']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "window = Window().partitionBy(F.col('user_id')).orderBy(F.col('user_id'))\n",
    "\n",
    "df_window = df.withColumn(\"min\", F.min(F.col('purchase_amount')).over(window)) \\\n",
    "              .withColumn(\"max\", F.max(F.col('purchase_amount')).over(window))\n",
    "\n",
    "df_window.select('dt', 'user_id', 'max', 'min').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .appName(\"ODS\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"/user/master/data/events/\")\n",
    "\n",
    "#df1 = df.filter(df.date <= F.to_date(F.lit(\"2022-05-31\"))) \n",
    "\n",
    "df1.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"date\", \"event_type\") \\\n",
    "    .save(\"/user/nickperegr/data/events/\")\n",
    "\n",
    "df2 = spark.read.parquet(\"/user/nickperegr/data/events/\")\n",
    "\n",
    "df2.select(\"event\", \"date\", \"event_type\") \\\n",
    "    .filter(F.col(\"event.datetime\").isNotNull()) \\\n",
    "    .orderBy(F.col(\"event.datetime\").desc()).show(10)\n",
    "\n",
    "df2.select(\"event\", \"date\", \"event_type\") \\\n",
    "    .filter(F.col(\"date\") <= F.to_date(F.lit(\"2022-05-31\"))) \\\n",
    "    .orderBy(F.col(\"event.datetime\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --master yarn --deploy-mode cluster partition.py 2022-05-31 /user/master/data/events /user/nickperegr/data/events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1c66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"test\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"/user/master/data/snapshots/tags_verified/actual\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(10);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def input_paths(date, depth):\n",
    "\n",
    "# Вычитаем дни из даты\n",
    "    end = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    start = end - timedelta(days=depth-1)\n",
    "    \n",
    "    date_generated = [start + timedelta(days=x) for x in range(0, (end-start).days+1)]\n",
    "    \n",
    "    paths = list()\n",
    "    \n",
    "    for date in date_generated:\n",
    "        tag_date = date.strftime(\"%Y-%m-%d\")\n",
    "        paths.append(f\"/user/nickperegr/data/events/date={tag_date}/event_type=message\")\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = input_paths('2022-05-31', 7)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37477794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def input_paths(date, depth):\n",
    "\n",
    "# Вычитаем дни из даты\n",
    "    end = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    start = end - timedelta(days=depth-1)\n",
    "    \n",
    "    date_generated = [start + timedelta(days=x) for x in range(0, (end-start).days+1)]\n",
    "    \n",
    "    paths = list()\n",
    "    \n",
    "    for date in date_generated:\n",
    "        tag_date = date.strftime(\"%Y-%m-%d\")\n",
    "        paths.append(f\"/user/nickperegr/data/events/date={tag_date}/event_type=message\")\n",
    "    \n",
    "    return paths\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"test\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "paths = input_paths('2022-05-31', 84)\n",
    "\n",
    "df = spark.read.parquet(*paths)\n",
    "\n",
    "#если в схеме есть массив, то нужно использовать explode\n",
    "df_comp = df.filter( F.col('event.message_channel_to').isNotNull())\\\n",
    "            .selectExpr('event.message_from as user', 'explode(event.tags) as tag')\\\n",
    "            .groupBy('tag')\\\n",
    "            .agg(F.countDistinct('user').alias('suggested_count'))\\\n",
    "            .where('suggested_count >= 2000')\n",
    "\n",
    "df_tags_verified = spark.read.parquet(\"/user/master/data/snapshots/tags_verified/actual\")\n",
    "\n",
    "df_joined = df_comp.join(df_tags_verified, [\"tag\"], how = \"left_anti\")\n",
    "\n",
    "df_joined.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"parquet\")\\\n",
    "    .save(\"/user/nickperegr/data/analytics/candidates_d84_pyspark\")\n",
    "\n",
    "df = spark.read.parquet('/user/nickperegr/data/analytics/candidates_d84_pyspark')\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83694ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def input_paths(date, depth):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return [f\"/user/nickperegr/data/events/date={(dt-datetime.timedelta(days=x)).strftime('%Y-%m-%d')}/event_type=message\" for x in range(depth)]\n",
    "\n",
    "paths = input_paths('2022-05-31', 7)\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"test\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "messages = spark.read.parquet(*paths)\n",
    "\n",
    "all_tags = messages.where(\"event.message_channel_to is not null\")\\\n",
    ".selectExpr([\"event.message_from as user\", \"explode(event.tags) as tag\"])\\\n",
    ".groupBy(\"tag\").agg(F.expr(\"count(distinct user) as suggested_count\"))\\\n",
    ".where(\"suggested_count >= 100\")\n",
    "\n",
    "#all_tags.orderBy(F.desc('suggested_count')).show()\n",
    "\n",
    "verified_tags = spark.read.parquet(\"/user/master/data/snapshots/tags_verified/actual\")\n",
    "candidates = all_tags.join(verified_tags, \"tag\", \"left_anti\")\n",
    "\n",
    "candidates.write.mode(\"overwrite\").parquet('/user/nickperegr/data/analytics/candidates_d7_pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ca59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"test\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.parquet('/user/nickperegr/data/analytics/candidates_d84_pyspark')\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5872865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def input_paths(date, depth, base_input_path):\n",
    "\n",
    "# Вычитаем дни из даты\n",
    "    end = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    start = end - timedelta(days=depth-1)\n",
    "    \n",
    "    date_generated = [start + timedelta(days=x) for x in range(0, (end-start).days+1)]\n",
    "    \n",
    "    paths = list()\n",
    "    \n",
    "    for date in date_generated:\n",
    "        tag_date = date.strftime(\"%Y-%m-%d\")\n",
    "        paths.append(f\"{base_input_path}/date={tag_date}/event_type=message\")\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = input_paths('2022-06-04', 5, '/user/nickperegr/data/events')\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d885a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"tag_tops\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "messages = spark.read.parquet(*paths)\n",
    "\n",
    "messages.printSchema()\n",
    "\n",
    "all_tags = messages.where(\"event.message_channel_to is not null\")\\\n",
    "    .selectExpr([\"event.message_id\", \"event.message_from as user_id\", \"explode(event.tags) as tag\"])\\\n",
    "    .groupBy(\"user_id\", \"tag\")\\\n",
    "    .agg(F.count(\"tag\").cast(LongType()).alias(\"suggested_count\"))\\\n",
    "\n",
    "window = Window.partitionBy(\"user_id\").orderBy(F.desc(\"tag\"), F.desc(\"suggested_count\"))\n",
    "\n",
    "df_pivoted = all_tags.withColumn(\"rn\", F.row_number().over(window))\\\n",
    "    .filter(F.col(\"rn\").cast(LongType()) <= 3)\\\n",
    "    .drop(\"suggested_count\")\\\n",
    "    .groupBy(\"user_id\").pivot(\"rn\").agg(F.first(\"tag\"))\n",
    "\n",
    "result = df_pivoted.withColumnRenamed(\"1\",\"tag_top_1\")\\\n",
    "           .withColumnRenamed(\"2\",\"tag_top_2\")\\\n",
    "           .withColumnRenamed(\"3\",\"tag_top_3\")\n",
    "    \n",
    "result.show(20, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"tag_tops\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "result = spark.read \\\n",
    "        .parquet(*paths) \\\n",
    "        .where(\"event.message_channel_to is not null\") \\\n",
    "        .select(F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "                F.col(\"event.message_from\").alias(\"user_id\"),\n",
    "                F.explode(F.col(\"event.tags\")).alias(\"tag\")) \\\n",
    "        .groupBy(\"user_id\", \"tag\") \\\n",
    "        .agg(F.count(\"*\").alias(\"tag_count\")) \\\n",
    "        .withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"user_id\") \\\n",
    "                                                .orderBy(F.desc(\"tag\"), F.desc(\"tag_count\")))) \\\n",
    "        .where(\"rank <= 3\") \\\n",
    "        .groupBy(\"user_id\") \\\n",
    "        .pivot(\"rank\", [1, 2, 3]) \\\n",
    "        .agg(F.first(\"tag\")) \\\n",
    "        .withColumnRenamed(\"1\", \"tag_top_1\") \\\n",
    "        .withColumnRenamed(\"2\", \"tag_top_2\") \\\n",
    "        .withColumnRenamed(\"3\", \"tag_top_3\")\n",
    "\n",
    "result.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c364676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"tag_tops\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "result = spark.read \\\n",
    "        .parquet(*paths) \\\n",
    "        .where(\"event.message_channel_to is not null\") \\\n",
    "        .select(F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "                F.col(\"event.message_from\").alias(\"user_id\"),\n",
    "                F.explode(F.col(\"event.tags\")).alias(\"tag\"))\\\n",
    "        .groupBy(\"user_id\", \"tag\") \\\n",
    "        .agg(F.count(\"*\").alias(\"tag_count\"))\\\n",
    "        .withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"user_id\") \\\n",
    "        .orderBy(F.desc(\"tag\"), F.desc(\"tag_count\"))))\n",
    "\n",
    "print(result.count())\n",
    "\n",
    "\n",
    "result1 = spark.read \\\n",
    "        .parquet(*paths) \\\n",
    "        .where(\"event.message_channel_to is not null\") \\\n",
    "        .select(F.col(\"event.message_from\").alias(\"user_id\"),\n",
    "                F.explode(F.col(\"event.tags\")).alias(\"tag\"))\\\n",
    "        .groupBy(\"user_id\", \"tag\") \\\n",
    "        .agg(F.count(\"*\").alias(\"tag_count\"))\\\n",
    "        .withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"user_id\") \\\n",
    "        .orderBy(F.desc(\"tag\"), F.desc(\"tag_count\"))))\n",
    "\n",
    "print(result1.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def input_event_paths(date, depth):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return [f\"/user/nickperegr/data/events/date={(dt - datetime.timedelta(days=x)).strftime('%Y-%m-%d')}\" for x in\n",
    "            range(depth)]\n",
    "\n",
    "\n",
    "def reaction_tag_tops(date, depth, spark):\n",
    "    reaction_paths = input_event_paths(date, depth)\n",
    "    reactions = spark.read \\\n",
    "        .option(\"basePath\", \"/user/nickperegr/data/events\") \\\n",
    "        .parquet(*reaction_paths) \\\n",
    "        .where(\"event_type='reaction'\")\n",
    "\n",
    "    all_message_tags = spark.read.parquet(\"/user/nickperegr/data/events\") \\\n",
    "        .where(\"event_type='message' and event.message_channel_to is not null\") \\\n",
    "        .select(F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "                F.col(\"event.message_from\").alias(\"user_id\"),\n",
    "                F.explode(F.col(\"event.tags\")).alias(\"tag\")\n",
    "                )\n",
    "\n",
    "    reaction_tags = reactions \\\n",
    "        .select(F.col(\"event.reaction_from\").alias(\"user_id\"),\n",
    "                F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "                F.col(\"event.reaction_type\").alias(\"reaction_type\")\n",
    "                ).join(all_message_tags.select(\"message_id\", \"tag\"), \"message_id\")\n",
    "\n",
    "    reaction_tops = reaction_tags \\\n",
    "        .groupBy(\"user_id\", \"tag\", \"reaction_type\") \\\n",
    "        .agg(F.count(\"*\").alias(\"tag_count\")) \\\n",
    "        .withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"user_id\", \"reaction_type\") \\\n",
    "                                                .orderBy(F.desc(\"tag\"), F.desc(\"tag_count\")))) \\\n",
    "        .where(\"rank <= 3\") \\\n",
    "        .groupBy(\"user_id\", \"reaction_type\") \\\n",
    "        .pivot(\"rank\", [1, 2, 3]) \\\n",
    "        .agg(F.first(\"tag\")) \\\n",
    "        .cache()\n",
    "\n",
    "    like_tops = reaction_tops \\\n",
    "        .where(\"reaction_type = 'like'\") \\\n",
    "        .drop(\"reaction_type\") \\\n",
    "        .withColumnRenamed(\"1\", \"like_tag_top_1\") \\\n",
    "        .withColumnRenamed(\"2\", \"like_tag_top_2\") \\\n",
    "        .withColumnRenamed(\"3\", \"like_tag_top_3\")\n",
    "\n",
    "    dislike_tops = reaction_tops \\\n",
    "        .where(\"reaction_type = 'dislike'\") \\\n",
    "        .drop(\"reaction_type\") \\\n",
    "        .withColumnRenamed(\"1\", \"dislike_tag_top_1\") \\\n",
    "        .withColumnRenamed(\"2\", \"dislike_tag_top_2\") \\\n",
    "        .withColumnRenamed(\"3\", \"dislike_tag_top_3\")\n",
    "\n",
    "    result = like_tops \\\n",
    "        .join(dislike_tops, \"user_id\", \"full_outer\")\n",
    "    print(\"here 1 \")\n",
    "    result.write.mode(\"overwrite\").parquet(f\"/user/nickperegr/data/tmp/reaction_tag_tops_{date[5:7]}_{date[8:10]}_{depth}\")\n",
    "    print(f\"save /user/nickperegr/data/tmp/reaction_tag_tops_{date[5:7]}_{date[8:10]}_{depth}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf5c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"reaction_tag_tops\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "depths=[5,5,1]\n",
    "dates=['2022-05-04', '2022-04-04', '2022-04-04']\n",
    "s = zip(dates,depths)\n",
    "for date,depth in s:\n",
    "    reaction_tag_tops(date, depth, spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bba880",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"2022-04-04\"\n",
    "print (message[-11111:2]+'_'+message[8:1111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def input_event_paths(date, depth):\n",
    "        dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "        return [f\"/user/nickperegr/data/events/date={(dt-datetime.timedelta(days=x)).strftime('%Y-%m-%d')}\" for x in range(depth)]\n",
    "def tag_tops(date, depth, spark):\n",
    "\t\n",
    "    message_paths = input_event_paths(date, depth)\n",
    "\t\n",
    "    result = spark.read\\\n",
    "    .option(\"basePath\", \"/user/nickperegr/data/events\")\\\n",
    "    .parquet(*message_paths)\\\n",
    "    .where(\"event_type = 'message'\")\\\n",
    "\t\t.where(\"event.message_channel_to is not null\")\\\n",
    "\t\t.select(F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "\t\t        F.col(\"event.message_from\").alias(\"user_id\"),\n",
    "\t\t        F.explode(F.col(\"event.tags\")).alias(\"tag\"))\\\n",
    "\t\t.groupBy(\"user_id\", \"tag\")\\\n",
    "\t\t.agg(F.count(\"*\").alias(\"tag_count\"))\\\n",
    "\t\t.withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"user_id\")\\\n",
    "        .orderBy(F.desc(\"tag\"), F.desc(\"tag_count\"))))\\\n",
    "\t\t.where(\"rank <= 3\")\\\n",
    "\t\t.groupBy(\"user_id\")\\\n",
    "\t\t.pivot(\"rank\", [1, 2, 3])\\\n",
    "\t\t.agg(F.first(\"tag\"))\\\n",
    "\t\t.withColumnRenamed(\"1\", \"tag_top_1\")\\\n",
    "\t\t.withColumnRenamed(\"2\", \"tag_top_2\")\\\n",
    "\t\t.withColumnRenamed(\"3\", \"tag_top_3\")\n",
    "\t\n",
    "    return result\n",
    "\n",
    "def reaction_tag_tops(date, depth, spark):\n",
    "\t\n",
    "    reaction_paths = input_event_paths(date, depth)\n",
    "\t\n",
    "    reactions = spark.read\\\n",
    "    .option(\"basePath\", \"/user/nickperegr/data/events\")\\\n",
    "    .parquet(*reaction_paths)\\\n",
    "    .where(\"event_type = 'reaction'\")\n",
    "\t\n",
    "    all_message_tags = spark.read.parquet(f\"{events_base_path}\")\\\n",
    "\t\t.where(\"event_type='message' and event.message_channel_to is not null\")\\\n",
    "\t\t.select(F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "\t\t        F.col(\"event.message_from\").alias(\"user_id\"),\n",
    "\t\t        F.explode(F.col(\"event.tags\")).alias(\"tag\")\n",
    "\t\t)\n",
    "\t\n",
    "    reaction_tags = reactions\\\n",
    "\t\t.select(F.col(\"event.reaction_from\").alias(\"user_id\"),\n",
    "\t\t        F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "\t\t        F.col(\"event.reaction_type\").alias(\"reaction_type\")\n",
    "\t\t).join(all_message_tags.select(\"message_id\", \"tag\"), \"message_id\")\n",
    "\t\n",
    "    reaction_tops = reaction_tags\\\n",
    "\t\t.groupBy(\"user_id\", \"tag\", \"reaction_type\")\\\n",
    "\t\t.agg(F.count(\"*\").alias(\"tag_count\"))\\\n",
    "\t\t.withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"user_id\", \"reaction_type\")\\\n",
    "        .orderBy(F.desc(\"tag\"), F.desc(\"tag_count\"))))\\\n",
    "\t\t.where(\"rank <= 3\")\\\n",
    "\t\t.groupBy(\"user_id\", \"reaction_type\")\\\n",
    "\t\t.pivot(\"rank\", [1, 2, 3])\\\n",
    "\t\t.agg(F.first(\"tag\"))\\\n",
    "\t\t.cache()\n",
    "\t\n",
    "    like_tops = reaction_tops\\\n",
    "\t\t.where(\"reaction_type = 'like'\")\\\n",
    "\t\t.drop(\"reaction_type\")\\\n",
    "\t\t.withColumnRenamed(\"1\", \"like_tag_top_1\")\\\n",
    "\t\t.withColumnRenamed(\"2\", \"like_tag_top_2\")\\\n",
    "\t\t.withColumnRenamed(\"3\", \"like_tag_top_3\")\n",
    "\t\n",
    "    dislike_tops = reaction_tops\\\n",
    "\t\t.where(\"reaction_type = 'dislike'\")\\\n",
    "\t\t.drop(\"reaction_type\")\\\n",
    "\t\t.withColumnRenamed(\"1\", \"dislike_tag_top_1\")\\\n",
    "\t\t.withColumnRenamed(\"2\", \"dislike_tag_top_2\")\\\n",
    "\t\t.withColumnRenamed(\"3\", \"dislike_tag_top_3\")\n",
    "\t\n",
    "    result = like_tops\\\n",
    "\t\t.join(dislike_tops, \"user_id\", \"full_outer\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_user_interests(date, depth, filename, spark):\n",
    "\n",
    "    post_tops = tag_tops(date, depth, spark)\n",
    "    \n",
    "    reaction_tops = reaction_tag_tops(date, depth, spark)\n",
    "    \n",
    "    result = post_tops.join(reaction_tops, \"user_id\", \"full_outer\")\n",
    "\n",
    "    result.write.mode(\"overwrite\").parquet(f\"/user/nickperegr/data/analytics/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28054432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"user_interests\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "depths=[5,5,1]\n",
    "dates=['2022-04-04', '2022-05-04', '2022-04-04']\n",
    "filenames=['user_interests_04_04_5', 'user_interests_04_05_5', 'user_interests_04_04_1']\n",
    "s = zip(dates,depths,filenames)\n",
    "for date,depth,filename in s:\n",
    "    calculate_user_interests(date, depth, filename, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"user_interests\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "depths=[7,28]\n",
    "dates=['2022-05-31', '2022-05-31']\n",
    "filenames=['user_interests_05_31_7', 'user_interests_05_31_28']\n",
    "s = zip(dates,depths,filenames)\n",
    "for date,depth,filename in s:\n",
    "    calculate_user_interests(date, depth, filename, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ca59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def input_event_paths(date, depth, events_base_path):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return [f\"{events_base_path}/date={(dt - datetime.timedelta(days=x)).strftime('%Y-%m-%d')}\" for x in\n",
    "            range(depth)]\n",
    "\n",
    "def connection_interests(date, days_count, events_base_path, spark):\n",
    "\t\n",
    "    message_paths = input_event_paths(f\"{date}\", int(days_count), f\"{events_base_path}\")\n",
    "\n",
    "    all_message_from = spark.read.parquet(f\"{events_base_path}\") \\\n",
    "        .where(\"event_type='message' and event.message_channel_to is not null\") \\\n",
    "        .select(F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "                F.col(\"event.message_from\").alias(\"user_id\"),\n",
    "                F.explode(F.col(\"event.tags\")).alias(\"tag\")\n",
    "                )\n",
    "\n",
    "    all_message_from.printSchema()\n",
    "    \n",
    "    all_message_from.show(20)    \n",
    "    \n",
    "    all_message_to = spark.read.parquet(f\"{events_base_path}\") \\\n",
    "        .where(\"event_type='message' and event.message_channel_to is not null\") \\\n",
    "        .select(F.col(\"event.message_id\").alias(\"message_id\"),\n",
    "                F.col(\"event.message_to\").alias(\"user_id\"),\n",
    "                F.explode(F.col(\"event.tags\")).alias(\"tag\")\n",
    "                )\n",
    "\n",
    "    all_message_to.printSchema()\n",
    "    \n",
    "    all_message_to.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bad2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"connection_interests\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "depths=[5]\n",
    "dates=['2022-05-25']\n",
    "b_paths=['/user/nickperegr/data/events']\n",
    "s = zip(dates,depths,b_paths)\n",
    "for date,depth,b_path in s:\n",
    "    connection_interests(date, depth, b_path, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6833c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .appName(\"test5\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "#spark.read.parquet(\"/user/master/data/snapshots/users/actual\").where(\"age > 18\").groupBy(\"id\").agg(F.avg(\"age\").alias(\"avgage\")).repartition(3).where(\"avgage < 50\").distinct().show()\n",
    "#spark.read.parquet(\"/user/master/data/snapshots/users/actual\").where(\"age > 18\").groupBy(\"id\").agg(F.avg(\"age\")).collect()\n",
    "#spark.read.parquet(\"/user/master/data/snapshots/users/actual\").where(\"age > 18\").groupBy(\"id\").agg(F.avg(\"age\").alias(\"avgage\")).show()\n",
    "\n",
    "df1 = spark.read.parquet(\"/user/examples/users_small\")\n",
    "\n",
    "F.broadcast(df1)\n",
    "\n",
    "df2 = spark.read.parquet(\"/user/nickperegr/data/events\")\n",
    "\n",
    "df2.printSchema()\n",
    "\n",
    "cond = [df1.id == df2.event.message_from]\n",
    "\n",
    "df = df1.join(df2, cond, \"inner\")\n",
    "\n",
    "df.explain(True)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bced859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\\\n",
    "        .appName(\"test6\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "#spark.read.parquet(\"/user/master/data/snapshots/users/actual\").where(\"age > 18\").groupBy(\"id\").agg(F.avg(\"age\").alias(\"avgage\")).repartition(3).where(\"avgage < 50\").distinct().show()\n",
    "#spark.read.parquet(\"/user/master/data/snapshots/users/actual\").where(\"age > 18\").groupBy(\"id\").agg(F.avg(\"age\")).collect()\n",
    "#spark.read.parquet(\"/user/master/data/snapshots/users/actual\").where(\"age > 18\").groupBy(\"id\").agg(F.avg(\"age\").alias(\"avgage\")).show()\n",
    "\n",
    "df1 = spark.read.parquet(\"/user/examples/users_small\")\n",
    "\n",
    "F.broadcast(df1)\n",
    "\n",
    "df2 = spark.read.parquet(\"/user/nickperegr/data/events\")\n",
    "\n",
    "df2.printSchema()\n",
    "\n",
    "cond = [df1.id == df2.event.message_from]\n",
    "\n",
    "df = df1.join(df2, cond, \"inner\")\n",
    "\n",
    "df.explain(True)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7cdadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\\\n",
    "        .appName(\"test6\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "user_bd = spark.read.parquet(\"/user/examples/user_birthdays\")\n",
    "\n",
    "events = spark.read.parquet(\"/user/nickperegr/data/events/\")\n",
    "\n",
    "res = user_bd.join(events.select(\"event.*\"), F.col(\"id\") == F.col(\"message_from\"), \"left_outer\")\\\n",
    ".groupBy(\"id\", \"birthday\").agg(F.count(F.when(F.expr(\"message like '%birthday%'\"), \n",
    "F.lit(1))).alias(\"count\"))\n",
    "\n",
    "#res.explain(True)\n",
    "\n",
    "res.write.mode(\"overwrite\").parquet(f\"/user/nickperegr/data/analytics/user_birthdays\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c23e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db08f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.cores\", 2) \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.cores\",2) \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\\\n",
    "        .appName(\"test6\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "user_bd = spark.read.parquet(\"/user/examples/user_birthdays\")\n",
    "\n",
    "events = spark.read.parquet(\"/user/nickperegr/data/events/\")\n",
    "\n",
    "res = user_bd\\\n",
    ".select(\"id\")\\\n",
    ".distinct()\\\n",
    ".join(events.select(\"event.*\"), F.col(\"id\") == F.col(\"message_from\"), \"inner\")\\\n",
    ".groupBy(\"id\").agg(F.count(F.when(F.expr(\"message like '%birthday%'\"), F.lit(1))).alias(\"count\"))\\\n",
    ".join(user_bd, \"id\", \"right_outer\") \n",
    "\n",
    "res.explain(True)\n",
    "\n",
    "res.write.mode(\"overwrite\").parquet(f\"/user/nickperegr/data/analytics/user_birthdays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab62e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .appName(\"Learning DataFrames\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.parquet(\"/user/master/data/geo/events\")\n",
    "\n",
    "events.printSchema()\n",
    "\n",
    "events.select(\"event_type\", \"event.subscription_user\", \"event.datetime\", \"event.message_ts\", \"lat\", \"lon\")\\\n",
    ".where(\"event_type == 'subscription' and subscription_user is not null and subscription_user = 165829\")\\\n",
    ".show(20, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3832207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 17:53:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lng: string (nullable = true)\n",
      "\n",
      "+---+----------+--------+--------+\n",
      "| id|      city|     lat|     lng|\n",
      "+---+----------+--------+--------+\n",
      "|  1|    Sydney| -33,865|151,2094|\n",
      "|  2| Melbourne|-37,8136|144,9631|\n",
      "|  3|  Brisbane|-27,4678|153,0281|\n",
      "|  4|     Perth|-31,9522|115,8589|\n",
      "|  5|  Adelaide|-34,9289|138,6011|\n",
      "|  6|Gold Coast|-28,0167|   153,4|\n",
      "|  7|Cranbourne|-38,0996|145,2834|\n",
      "|  8|  Canberra|-35,2931|149,1269|\n",
      "|  9| Newcastle|-32,9167|  151,75|\n",
      "| 10|Wollongong|-34,4331|150,8831|\n",
      "| 11|   Geelong|  -38,15|  144,35|\n",
      "| 12|    Hobart|-42,8806| 147,325|\n",
      "| 13|Townsville|-19,2564|146,8183|\n",
      "| 14|   Ipswich|-27,6167|152,7667|\n",
      "| 15|    Cairns|-16,9303|145,7703|\n",
      "| 16| Toowoomba|-27,5667|  151,95|\n",
      "| 17|    Darwin|-12,4381|130,8411|\n",
      "| 18|  Ballarat|  -37,55|  143,85|\n",
      "| 19|   Bendigo|  -36,75|144,2667|\n",
      "| 20|Launceston|-41,4419| 147,145|\n",
      "+---+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Learning DataFrames\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "cities = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"true\").load(\"/user/nickperegr/data/cities\")\n",
    "\n",
    "cities.printSchema()\n",
    "\n",
    "cities.show(20)\n",
    "\n",
    "cities.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"/user/nickperegr/data/cities_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9b4042e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.window import Window\n",
    "import sys\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "#Вспомогательные функции - определение временных зон, загрузка справочников городов, получение локального времени\n",
    "def create_tz_dataset(spark):\n",
    "\n",
    "    schema = StructType([\n",
    "    StructField(\"city\", StringType(), nullable=False),\n",
    "    StructField(\"tz_city\", StringType(), nullable=False),])\n",
    "\n",
    "    tz = [\n",
    "            ('Sydney', 'Sydney'),\n",
    "            ('Melbourne', 'Melbourne'),\n",
    "            ('Brisbane', 'Brisbane'),\n",
    "            ('Perth', 'Perth'),\n",
    "            ('Adelaide', 'Adelaide'),\n",
    "            ('Gold Coast', 'Brisbane'),\n",
    "            ('Cranbourne', 'Melbourne'),\n",
    "            ('Canberra', 'Canberra'),\n",
    "            ('Newcastle', 'Sydney'),\n",
    "            ('Wollongong', 'Sydney'),\n",
    "            ('Geelong', 'Melbourne'),\n",
    "            ('Hobart', 'Hobart'),\n",
    "            ('Townsville', 'Brisbane'),\n",
    "            ('Ipswich', 'Brisbane'),\n",
    "            ('Cairns', 'Brisbane'),\n",
    "            ('Toowoomba', 'Brisbane'),\n",
    "            ('Darwin', 'Darwin'),\n",
    "            ('Ballarat', 'Melbourne'),\n",
    "            ('Bendigo', 'Melbourne'),\n",
    "            ('Launceston', 'Hobart'),\n",
    "            ('Mackay', 'Brisbane'),\n",
    "            ('Rockhampton', 'Brisbane'),\n",
    "            ('Maitland', 'Sydney' ),\n",
    "            ('Bunbury', 'Perth')\n",
    "    ]\n",
    "\n",
    "    tz_df = spark.createDataFrame(data=tz, schema=schema)\n",
    "\n",
    "    return tz_df\n",
    "\n",
    "def load_cities(spark):\n",
    "    cities = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"true\").load(\"/user/nickperegr/data/cities\") \\\n",
    "                  .withColumn(\"lat1\", F.regexp_replace(F.col(\"lat\"), ',', '.').cast(\"float\"))\\\n",
    "                  .withColumn(\"lon1\", F.regexp_replace(F.col(\"lng\"), ',', '.').cast(\"float\"))\\\n",
    "                  .drop(\"lat\", \"lng\")\n",
    "\n",
    "    return cities\n",
    "\n",
    "def load_local_time(df, left_join_col, time_col, spark):\n",
    "\n",
    "    tz = create_tz_dataset(spark)\n",
    "    \n",
    "    df_tz = df.join(tz, df[left_join_col] == tz[\"city\"], \"inner\").drop(tz.city).withColumnRenamed(\"tz_city\", \"city\")\n",
    "    \n",
    "    local_time = df_tz.withColumn(\"ts_timestamp\",F.col(time_col).cast(\"Timestamp\"))\\\n",
    "    .withColumn(\"timezone\",F.concat(F.lit(\"Australia/\"),F.col(\"city\")))\\\n",
    "    .withColumn(\"local_time\",F.from_utc_timestamp(F.col(\"ts_timestamp\"),F.col(\"timezone\")))\\\n",
    "    .drop(\"timezone\", \"ts_timestamp\")    \n",
    "    \n",
    "    return local_time\n",
    "\n",
    "#Витрина пользователей\n",
    "def load_geo_events_cities(date, days_delta, events_base_path, output_base_path, spark):\n",
    "\n",
    "    geo_df = load_cities(spark)\n",
    "\n",
    "    F.broadcast(geo_df)\n",
    "\n",
    "    end_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    start_date = end_date - timedelta(days=int(days_delta))\n",
    "\n",
    "    geo_events_base = spark.read.parquet(events_base_path)\\\n",
    "                   .filter(F.col(\"date\").between(start_date, end_date)) \n",
    "\n",
    "    geo_events = geo_events_base\\\n",
    "    .withColumn(\"user_id\",F.when(F.col(\"event_type\")==\"message\",F.col(\"event.message_from\"))\\\n",
    "                    .when(F.col(\"event_type\")==\"reaction\",F.col(\"event.reaction_from\"))\\\n",
    "                    .when(F.col(\"event_type\")==\"subscription\", F.coalesce(F.col(\"event.subscription_user\"),F.col(\"event.user\"))))\\\n",
    "    .withColumn(\"ts\",F.when(F.col(\"event_type\")==\"message\",F.coalesce(F.col(\"event.message_ts\"),F.col(\"event.datetime\")))\\\n",
    "                    .when(F.col(\"event_type\")==\"reaction\",F.col(\"event.datetime\"))\\\n",
    "                    .when(F.col(\"event_type\")==\"subscription\",F.col(\"event.datetime\")))\\\n",
    "    .filter(F.col(\"user_id\").isNotNull())\n",
    "\n",
    "    window = Window().partitionBy(\"user_id\", \"ts\").orderBy(\"distance\")\n",
    "\n",
    "    geo_events_cities=geo_events.crossJoin(geo_df)\\\n",
    "        .where(\"lat is not null and lon is not null\")\\\n",
    "        .withColumn(\"ts\", F.to_timestamp(F.substring(F.col(\"ts\"),1,19),\"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "        .withColumn(\"distance\", 2*6378*F.sqrt(F.pow(F.sin((F.col(\"lat1\")-F.col(\"lat\"))/2),2) + F.cos(\"lat\")*F.cos(\"lat1\")*F.pow(F.sin((F.col(\"lon1\")-F.col(\"lon\"))/2),2)))\\\n",
    "        .withColumn(\"rn\", F.row_number().over(window))\\\n",
    "        .where(\"rn=1\")\\\n",
    "        .drop(\"lat1\", \"lon1\", \"rn\", \"id\", \"distance\")\\\n",
    "        .persist()\n",
    "\n",
    "    geo_events_cities.write.mode(\"overwrite\").partitionBy(\"date\").parquet(f\"{output_base_path}/geo_events_cities\")\n",
    "\n",
    "    return geo_events_cities\n",
    "\n",
    "def load_home_cities(df):\n",
    "\n",
    "    window = Window().partitionBy(\"user_id\", \"city\").orderBy(\"date\") \n",
    "\n",
    "    window1 = Window().partitionBy(\"user_id\").orderBy(\"date\")\n",
    "    \n",
    "    window2 = Window().partitionBy(\"user_id\").orderBy(\"ts\") \n",
    "\n",
    "    home_cities = df\\\n",
    "      .select(\"user_id\", \"ts\", \"date\", \"city\")\\\n",
    "      .withColumn(\"days_rank\", F.dense_rank().over(window))\\\n",
    "      .withColumn(\"start_dt\",F.min(F.col(\"date\")).over(window))\\\n",
    "      .withColumn(\"end_dt\",F.max(F.col(\"date\")).over(window))\\\n",
    "      .withColumn(\"days\", F.datediff(F.col(\"end_dt\"),F.col(\"start_dt\"))+1)\\\n",
    "      .withColumn(\"prev_city\", F.lag(\"city\", 1, \"UnknownCity\").over(window2))\\\n",
    "      .withColumn(\"home_city\",F.when((F.col(\"days\")>=27) & (F.col(\"days\")==F.col(\"days_rank\")),F.col(\"city\")))\\\n",
    "      .withColumn(\"home_city\", F.last(\"home_city\", ignorenulls=True).over(window1))\\\n",
    "      .filter(F.col(\"city\") != F.col(\"prev_city\"))\\\n",
    "      .withColumn(\"travel_count\",F.size(F.collect_list(\"city\").over(window2)))\\\n",
    "      .withColumn(\"travel_array\", F.collect_list(\"city\").over(window2))\\\n",
    "      .withColumn(\"rn\",F.row_number().over(Window.partitionBy(\"user_id\").orderBy(F.col(\"ts\").desc())))\\\n",
    "      .filter(F.col(\"rn\") == 1)\\\n",
    "      .select(\"user_id\", \"home_city\", \"travel_count\", \"travel_array\")\n",
    "\n",
    "    return home_cities\n",
    "\n",
    "def load_geo_actual_cities(df):\n",
    "\n",
    "    window = Window().partitionBy(\"user_id\").orderBy(F.desc(\"ts\"))\n",
    "\n",
    "    geo_events_actual_cities = df\\\n",
    "        .withColumn(\"rn\", F.row_number().over(window))\\\n",
    "        .where(\"rn == 1\")\\\n",
    "        .withColumnRenamed(\"city\", \"act_city\")\\\n",
    "        .withColumnRenamed(\"ts\", \"local_time\")\\\n",
    "        .select(\"user_id\", \"local_time\", \"act_city\")\n",
    "\n",
    "    return geo_events_actual_cities\n",
    "\n",
    "def load_geo_dm(date, days_count, events_base_path, output_base_path, output_mart_path, spark):\n",
    "    geo_events_cities=load_geo_events_cities(date, days_count, events_base_path, output_base_path, spark)\n",
    "    home_cities = load_home_cities(geo_events_cities)\n",
    "    geo_events_actual_cities = load_geo_actual_cities(geo_events_cities)\n",
    "    local_time_cities = load_local_time(geo_events_actual_cities, \"act_city\", \"local_time\", spark)\n",
    "\n",
    "    cond = [home_cities.user_id == local_time_cities.user_id]\n",
    "    user_geo = home_cities.join(local_time_cities, cond, \"left\")\\\n",
    "                             .drop(local_time_cities.user_id)\\\n",
    "                             .select(\"user_id\", \"act_city\", \"home_city\", \"travel_count\", \"travel_array\", \"local_time\")\n",
    "\n",
    "    user_geo.write.mode(\"overwrite\").parquet(f\"{output_mart_path}/user_geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6e0bac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .appName(\"Project7\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "depths=[5]\n",
    "dates=['2022-05-01']\n",
    "b_paths=['/user/master/data/geo/events/']\n",
    "o_paths=['/user/nickperegr/prj7/prod/data/events/']\n",
    "m_paths=['/user/nickperegr/prj7/prod/data/analytics/']\n",
    "\n",
    "#s = zip(dates,depths,b_paths, o_paths, m_paths)\n",
    "#for date,depth,b_path, o_path, m_path in s:\n",
    "    #geo_events_cities=load_geo_events_cities(date, depth, b_path, o_path, spark)\n",
    "    #load_dm(date, depth, b_path, o_path, m_path, spark)\n",
    "#geo_events_cities = spark.read.parquet(\"/user/nickperegr/prj7/prod/data/events/\")    \n",
    "#home_cities = load_home_cities(geo_events_cities)    \n",
    "#geo_events_actual_cities = load_geo_actual_cities(geo_events_cities)\n",
    "local_time_cities = load_local_time(geo_events_actual_cities, \"act_city\", \"local_time\", spark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_events_cities.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_events_cities.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e462ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "| 106506|    7|\n",
      "| 112480|    5|\n",
      "| 127075|    4|\n",
      "| 157380|    6|\n",
      "| 169689|    6|\n",
      "|  18556|   13|\n",
      "|  56315|    3|\n",
      "|  57085|    5|\n",
      "|  65348|   11|\n",
      "|  70962|   10|\n",
      "|  80422|   12|\n",
      "|   8304|    5|\n",
      "| 115640|    8|\n",
      "| 148140|    5|\n",
      "| 168278|    4|\n",
      "|   2110|    9|\n",
      "|  21645|    3|\n",
      "|  35771|    9|\n",
      "|  39755|    5|\n",
      "|  49960|    9|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geo_events_cities.groupBy(\"user_id\").count()\\\n",
    "  .filter(F.col(\"count\") >= 2)\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39d10197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:==========================================>           (158 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+-------+-------------------+----------+------------------+\n",
      "|               event|  event_type|      date|user_id|                 ts|      city|          distance|\n",
      "+--------------------+------------+----------+-------+-------------------+----------+------------------+\n",
      "|{null, null, null...|     message|2022-04-26|  18556|2021-04-26 03:33:01|Wollongong|1981.8815744285155|\n",
      "|{null, null, 2022...|subscription|2022-04-26|  18556|2022-04-26 02:29:11| Newcastle| 497.9938087946334|\n",
      "|{null, null, 2022...|subscription|2022-04-26|  18556|2022-04-26 06:35:52|  Adelaide|1325.6469846287732|\n",
      "|{null, null, 2022...|subscription|2022-04-26|  18556|2022-04-26 11:31:36| Newcastle|1681.0194460714886|\n",
      "|{null, null, 2022...|subscription|2022-04-26|  18556|2022-04-26 13:34:39| Newcastle| 253.7102883924431|\n",
      "|{null, null, 2022...|subscription|2022-04-26|  18556|2022-04-26 21:50:30|  Adelaide|1438.8911331305167|\n",
      "|{null, null, 2022...|    reaction|2022-04-27|  18556|2022-04-27 12:00:32|   Bunbury|1584.2073389955838|\n",
      "|{null, null, 2022...|    reaction|2022-04-27|  18556|2022-04-27 12:06:52| Newcastle| 2557.807201533752|\n",
      "|{null, null, 2022...|subscription|2022-04-27|  18556|2022-04-27 23:47:40| Newcastle| 700.7255318930514|\n",
      "|{null, null, 2022...|subscription|2022-04-28|  18556|2022-04-28 10:11:12|Launceston| 966.6572802835616|\n",
      "|{null, null, 2022...|subscription|2022-04-28|  18556|2022-04-28 13:50:08| Toowoomba|2027.0576936279042|\n",
      "|{null, null, 2022...|    reaction|2022-04-30|  18556|2022-04-30 04:35:08|  Brisbane| 1165.734516296765|\n",
      "|{null, null, 2022...|subscription|2022-04-30|  18556|2022-04-30 09:43:24| Melbourne|2873.6543668575146|\n",
      "+--------------------+------------+----------+-------+-------------------+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geo_events_cities.drop(\"lat\", \"lon\").where(\"user_id= 18556\").orderBy(\"ts\").show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f64460fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_cities = load_home_cities(geo_events_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e4b9ac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:===================================================> (196 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|user_id|home_city|travel_count|travel_array                                                                                                          |\n",
      "+-------+---------+------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|18556  |Newcastle|11          |[Wollongong, Newcastle, Adelaide, Newcastle, Adelaide, Bunbury, Newcastle, Launceston, Toowoomba, Brisbane, Melbourne]|\n",
      "+-------+---------+------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "home_cities.where(\"user_id= 18556\").orderBy(\"ts\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "104e820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_events_actual_cities = load_geo_actual_cities(geo_events_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "426d66bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 169:=============================================>       (173 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+\n",
      "|user_id|         local_time| act_city|\n",
      "+-------+-------------------+---------+\n",
      "|  18556|2022-04-30 09:43:24|Melbourne|\n",
      "+-------+-------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geo_events_actual_cities.where(\"user_id= 18556\").orderBy(\"rn\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "514b426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 182:===========> (183 + 1) / 200][Stage 183:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+\n",
      "|user_id|         local_time| act_city|\n",
      "+-------+-------------------+---------+\n",
      "|  18556|2022-04-30 19:43:24|Melbourne|\n",
      "+-------+-------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "local_time_cities.where(\"user_id= 18556\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "148003c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [home_cities.user_id == local_time_cities.user_id]\n",
    "user_geo_events = home_cities.join(local_time_cities, cond, \"left\")\\\n",
    "                             .drop(local_time_cities.user_id)\\\n",
    "                             .select(\"user_id\", \"act_city\", \"home_city\", \"travel_count\", \"travel_array\", \"local_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f4f9a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- act_city: string (nullable = true)\n",
      " |-- home_city: string (nullable = true)\n",
      " |-- travel_count: integer (nullable = false)\n",
      " |-- travel_array: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- local_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_geo_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3f4f3a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+------------+----------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|user_id|act_city |home_city|travel_count|travel_array                                                                                                          |local_time         |\n",
      "+-------+---------+---------+------------+----------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|18556  |Melbourne|Newcastle|11          |[Wollongong, Newcastle, Adelaide, Newcastle, Adelaide, Bunbury, Newcastle, Launceston, Toowoomba, Brisbane, Melbourne]|2022-04-30 19:43:24|\n",
      "+-------+---------+---------+------------+----------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_geo_events.where(\"user_id= 18556\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30228a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 188:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------------+--------------------+-------------------+\n",
      "|user_id|  act_city|home_city|travel_count|        travel_array|         local_time|\n",
      "+-------+----------+---------+------------+--------------------+-------------------+\n",
      "|      0|   Bendigo|     null|           5|[Toowoomba, Canbe...|2022-05-30 21:57:17|\n",
      "|      1|  Canberra|     null|           5|[Toowoomba, Canbe...|2022-05-30 17:38:52|\n",
      "|     10| Melbourne|     null|           3|[Bunbury, Melbour...|2022-05-28 21:39:34|\n",
      "|    100| Toowoomba|     null|           7|[Toowoomba, Sydne...|2022-05-31 05:19:23|\n",
      "|   1000|Wollongong|     null|           4|[Wollongong, Rock...|2022-05-31 09:34:28|\n",
      "|  10000|Launceston|     null|           4|[Canberra, Adelai...|2022-05-31 07:32:35|\n",
      "| 100000| Toowoomba|     null|           3|[Toowoomba, Launc...|2022-05-26 11:44:53|\n",
      "| 100001|    Mackay|     null|           9|[Perth, Adelaide,...|2022-05-30 19:51:35|\n",
      "| 100002|    Sydney|     null|           4|[Sydney, Bunbury,...|2022-05-30 14:02:42|\n",
      "| 100003|    Hobart|     null|           5|[Adelaide, Rockha...|2022-05-31 00:06:11|\n",
      "+-------+----------+---------+------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_geo_events.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd747053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- act_city: string (nullable = true)\n",
      " |-- home_city: string (nullable = true)\n",
      " |-- travel_count: integer (nullable = true)\n",
      " |-- travel_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- local_time: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 131:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------------+--------------------+-------------------+\n",
      "|user_id|  act_city|home_city|travel_count|        travel_array|         local_time|\n",
      "+-------+----------+---------+------------+--------------------+-------------------+\n",
      "|      0|  Adelaide|     null|           4|[Adelaide, Bendig...|2022-04-30 18:45:11|\n",
      "|      0|  Adelaide|     null|           2|   [Bendigo, Mackay]|2022-04-30 18:45:11|\n",
      "|      0|  Adelaide|     null|           2|   [Bendigo, Mackay]|2022-04-30 18:45:11|\n",
      "|      0|  Adelaide|     null|           3|[Bendigo, Newcast...|2022-04-30 18:45:11|\n",
      "|     10|    Sydney|     null|           3|[Launceston, Bend...|2022-05-02 08:56:34|\n",
      "|     10|    Sydney|     null|           2|[Launceston, Town...|2022-05-02 08:56:34|\n",
      "|     10|    Sydney|     null|           4|[Sydney, Launcest...|2022-05-02 08:56:34|\n",
      "|     10|    Sydney|     null|           4|[Sydney, Launcest...|2022-05-02 08:56:34|\n",
      "|     10|    Sydney|     null|           1|        [Townsville]|2022-05-02 08:56:34|\n",
      "|    100|Launceston|     null|           2|[Launceston, Rock...|2022-04-29 06:46:43|\n",
      "+-------+----------+---------+------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .appName(\"Project 7\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.parquet(\"/user/nickperegr/prj7/prod/data/analytics/user_geo/\")\n",
    "\n",
    "events.printSchema()\n",
    "\n",
    "#events.where(\"home_city is not null\").show(20, False)\n",
    "events.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d57562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+------------+------------+----------+\n",
      "|user_id|act_city|home_city|travel_count|travel_array|local_time|\n",
      "+-------+--------+---------+------------+------------+----------+\n",
      "+-------+--------+---------+------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.where(\"home_city is not null\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca1c9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def load_month_week_stat(input_events_path, spark):\n",
    "    \n",
    "    window_reg = Window.partitionBy(\"user_id\",\"event_type\").orderBy(\"ts\")\n",
    "    \n",
    "    geo_events = spark.read \\\n",
    "     .parquet(f\"{input_events_path}\")\n",
    "    \n",
    "    \n",
    "    events = geo_events.select(F.col(\"city\").alias(\"zone_id\"),\\\n",
    "                                    \"event_type\", \"user_id\", \"date\", \"ts\")\\\n",
    "     .where(\"user_id= 100310\")\\\n",
    "     .withColumn(\"message_number\",F.row_number().over(window_reg)) \\\n",
    "     .withColumn(\"month\",F.trunc(F.col(\"date\"), \"month\"))\\\n",
    "     .withColumn(\"week\",F.trunc(F.col(\"date\"), \"week\"))\\\n",
    "     .drop(\"user_id\", \"date\", \"ts\")\n",
    "    \n",
    "    events_stat_m = events\\\n",
    "     .groupBy(\"zone_id\", \"month\")\\\n",
    "     .agg(\n",
    "         F.sum(F.when(geo_events.event_type == \"message\",1).otherwise(0)).alias(\"month_message\"),\n",
    "         F.sum(F.when(geo_events.event_type == \"reaction\",1).otherwise(0)).alias(\"month_reaction\"),\n",
    "         F.sum(F.when(geo_events.event_type == \"subscription\",1).otherwise(0)).alias(\"month_subscription\"),\n",
    "         F.sum(F.when((geo_events.event_type == \"message\") & (F.col(\"message_number\")==1 ),1).otherwise(0)).alias(\"month_user\")\n",
    "    )\n",
    "    \n",
    "    events_stat_w = events\\\n",
    "     .groupBy(\"zone_id\", \"month\", \"week\")\\\n",
    "     .agg(\n",
    "         F.sum(F.when(geo_events.event_type == \"message\",1).otherwise(0)).alias(\"week_message\"),\n",
    "         F.sum(F.when(geo_events.event_type == \"reaction\",1).otherwise(0)).alias(\"week_reaction\"),\n",
    "         F.sum(F.when(geo_events.event_type == \"subscription\",1).otherwise(0)).alias(\"week_subscription\"),\n",
    "         F.sum(F.when((geo_events.event_type == \"message\") & (F.col(\"message_number\")==1 ),1).otherwise(0)).alias(\"week_user\")\n",
    "    )\n",
    "    \n",
    "    cond = [events_stat_m.zone_id == events_stat_w.zone_id, events_stat_m.month == events_stat_w.month]\n",
    "    events_stat = events_stat_m.join(events_stat_w, cond, \"left\")\\\n",
    "                               .drop(events_stat_w.zone_id)\\\n",
    "                               .drop(events_stat_w.month)\n",
    "    \n",
    "    return events_stat\n",
    "\n",
    "def load_stat_dm(input_events_path, output_mart_path, spark):\n",
    "    \n",
    "    events_stat = load_month_week_stat(input_events_path, spark)\n",
    "\n",
    "    events_stat.write.mode(\"overwrite\").parquet(f\"{output_mart_path}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "439e4b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 16:27:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .appName(\"Project7\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "i_paths=['/user/nickperegr/prj7/prod/data/events/geo_events_cities/']\n",
    "m_paths=['/user/nickperegr/prj7/prod/data/analytics/events_stat']\n",
    "\n",
    "s = zip(i_paths, m_paths)\n",
    "for i_path, m_path in s:\n",
    "    #events_stat = load_month_week_stat(i_path, spark)\n",
    "    load_stat_dm(i_path, m_path, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50969757",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_stat.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb544dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_stat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_stat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f276b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_stat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62788a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 16:27:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "root\n",
      " |-- month_message: long (nullable = true)\n",
      " |-- month_reaction: long (nullable = true)\n",
      " |-- month_subscription: long (nullable = true)\n",
      " |-- month_user: long (nullable = true)\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- month: date (nullable = true)\n",
      " |-- week: date (nullable = true)\n",
      " |-- week_message: long (nullable = true)\n",
      " |-- week_reaction: long (nullable = true)\n",
      " |-- week_subscription: long (nullable = true)\n",
      " |-- week_user: long (nullable = true)\n",
      "\n",
      "+-------------+--------------+------------------+----------+-----------+----------+----------+------------+-------------+-----------------+---------+\n",
      "|month_message|month_reaction|month_subscription|month_user|    zone_id|     month|      week|week_message|week_reaction|week_subscription|week_user|\n",
      "+-------------+--------------+------------------+----------+-----------+----------+----------+------------+-------------+-----------------+---------+\n",
      "|            0|             0|                 1|         0|Rockhampton|2022-05-01|2022-04-25|           0|            0|                1|        0|\n",
      "+-------------+--------------+------------------+----------+-----------+----------+----------+------------+-------------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .appName(\"Project 7\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.parquet(\"/user/nickperegr/prj7/prod/data/analytics/events_stat/\")\n",
    "\n",
    "events.printSchema()\n",
    "\n",
    "#events.where(\"home_city is not null\").show(20, False)\n",
    "events.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cd6a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def create_tz_dataset(spark):\n",
    "\n",
    "    schema = StructType([\n",
    "    StructField(\"city\", StringType(), nullable=False),\n",
    "    StructField(\"tz_city\", StringType(), nullable=False),])\n",
    "\n",
    "    tz = [\n",
    "            ('Sydney', 'Sydney'),\n",
    "            ('Melbourne', 'Melbourne'),\n",
    "            ('Brisbane', 'Brisbane'),\n",
    "            ('Perth', 'Perth'),\n",
    "            ('Adelaide', 'Adelaide'),\n",
    "            ('Gold Coast', 'Brisbane'),\n",
    "            ('Cranbourne', 'Melbourne'),\n",
    "            ('Canberra', 'Canberra'),\n",
    "            ('Newcastle', 'Sydney'),\n",
    "            ('Wollongong', 'Sydney'),\n",
    "            ('Geelong', 'Melbourne'),\n",
    "            ('Hobart', 'Hobart'),\n",
    "            ('Townsville', 'Brisbane'),\n",
    "            ('Ipswich', 'Brisbane'),\n",
    "            ('Cairns', 'Brisbane'),\n",
    "            ('Toowoomba', 'Brisbane'),\n",
    "            ('Darwin', 'Darwin'),\n",
    "            ('Ballarat', 'Melbourne'),\n",
    "            ('Bendigo', 'Melbourne'),\n",
    "            ('Launceston', 'Hobart'),\n",
    "            ('Mackay', 'Brisbane'),\n",
    "            ('Rockhampton', 'Brisbane'),\n",
    "            ('Maitland', 'Sydney' ),\n",
    "            ('Bunbury', 'Perth')\n",
    "    ]\n",
    "\n",
    "    tz_df = spark.createDataFrame(data=tz, schema=schema)  \n",
    "\n",
    "    return tz_df\n",
    "\n",
    "\n",
    "def load_local_time(df, left_join_col, time_col, spark):\n",
    "\n",
    "    tz = create_tz_dataset(spark)\n",
    "    \n",
    "    df_tz = df.join(tz, df[left_join_col] == tz[\"city\"], \"inner\").drop(tz.city).withColumnRenamed(\"tz_city\", \"city\")\n",
    "    \n",
    "    local_time = df_tz.withColumn(\"ts_timestamp\",F.col(f\"{time_col}\").cast(\"Timestamp\"))\\\n",
    "    .withColumn(\"timezone\",F.concat(F.lit(\"Australia/\"),F.col(\"city\")))\\\n",
    "    .withColumn(\"local_time\",F.from_utc_timestamp(F.col(\"ts_timestamp\"),F.col(\"timezone\")))\n",
    "    \n",
    "    return local_time\n",
    "\n",
    "\n",
    "def load_recommendations(input_events_path, spark):\n",
    "    \n",
    "    geo_events = spark.read \\\n",
    "     .parquet(f\"{input_events_path}\")\n",
    "    \n",
    "    subscription_events=geo_events\\\n",
    "            .filter(\"event_type='subscription'\")\\\n",
    "            .select(\"user_id\", \"event.subscription_channel\")\\\n",
    "            .distinct()\n",
    "    \n",
    "    messages = geo_events\\\n",
    "            .where(\"event_type='message' and event.message_to is not null\")\\\n",
    "            .select(F.col(\"user_id\").alias(\"message_from\"), F.col(\"event.message_to\").alias(\"message_to\"))\\\n",
    "            .distinct()\n",
    "\n",
    "    contacts = messages\\\n",
    "            .union(messages.select(\"message_to\", \"message_from\"))\\\n",
    "            .distinct()\\\n",
    "            .withColumn(\"is_contacted\", F.lit(1))\\\n",
    "    \n",
    "    pair_users = subscription_events.alias(\"p1\")\\\n",
    "            .join(subscription_events.alias(\"p2\"),\"subscription_channel\",\"inner\")\\\n",
    "            .where(\"p1.user_id != p2.user_id\")\\\n",
    "            .withColumn(\"user_left\", F.when(F.col(\"p1.user_id\")>F.col(\"p2.user_id\"), F.col(\"p1.user_id\"))\\\n",
    "                                                             .otherwise(F.col(\"p2.user_id\")))\\\n",
    "            .withColumn(\"user_right\", F.when(F.col(\"p1.user_id\")>F.col(\"p2.user_id\"), F.col(\"p2.user_id\"))\\\n",
    "                                                             .otherwise(F.col(\"p1.user_id\")))\\\n",
    "            .select(\"user_left\", \"user_right\")\\\n",
    "            .distinct()\n",
    "    \n",
    "    pair_users = pair_users.join(contacts,[F.col(\"user_left\") == F.col(\"message_from\"),\n",
    "                                           F.col(\"user_right\") == F.col(\"message_to\")], \"leftanti\")\n",
    "    \n",
    "    events_left = geo_events.select(F.col(\"user_id\").alias(\"user_left\"), \n",
    "                                    \"date\", \n",
    "                                    \"ts\",\n",
    "                                    \"lat\",\n",
    "                                    \"lon\",\n",
    "                                    F.col(\"city\").alias(\"zone_id\"))\n",
    "                       \n",
    "    events_right = geo_events.select(F.col(\"user_id\").alias(\"user_right\"),\n",
    "                                    \"date\",\n",
    "                                     F.col(\"lat\").alias(\"lat1\"),\n",
    "                                     F.col(\"lon\").alias(\"lon1\"),\n",
    "                                     F.col(\"city\").alias(\"zone2_id\"))                  \n",
    "    \n",
    "    events_all=pair_users.join(events_left, \"user_left\", \"left\")\\\n",
    "                              .join(events_right, [\"user_right\",\"date\"], \"left\")\\\n",
    "                              .withColumn(\"distance\", 2*6378*F.sqrt(F.pow(F.sin((F.col(\"lat1\")-F.col(\"lat\"))/2),2) + F.cos(\"lat\")*F.cos(\"lat1\")*F.pow(F.sin((F.col(\"lon1\")-F.col(\"lon\"))/2),2)))\\\n",
    "                              .filter(F.col(\"distance\")<1)\\\n",
    "                              .select(\"user_left\",\"user_right\",\\\n",
    "                                      F.current_timestamp().alias(\"processed_dttm\"),\\\n",
    "                                      \"zone_id\")\\\n",
    "                              .distinct()        \n",
    "    \n",
    "    recommendations = load_local_time(events_all, \"zone_id\", \"processed_dttm\", spark)\\\n",
    "                            .select(\"user_left\", \"user_right\", \"processed_dttm\", \"zone_id\", \"local_time\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def load_recommendations_dm(input_events_path, output_mart_path, spark):\n",
    "\n",
    "    recommendations = load_recommendations(input_events_path, spark)\n",
    "\n",
    "    recommendations.write.mode(\"overwrite\").parquet(f\"{output_mart_path}\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f24c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .appName(\"Project7\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "i_paths=['/user/nickperegr/prj7/prod/data/events/geo_events_cities/']\n",
    "m_paths=['/user/nickperegr/prj7/prod/data/analytics/recommendations']\n",
    "\n",
    "s = zip(i_paths, m_paths)\n",
    "for i_path, m_path in s:\n",
    "    load_recommendations_dm(i_path, m_path, spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7cdcb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 19:57:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "root\n",
      " |-- user_left: string (nullable = true)\n",
      " |-- user_right: string (nullable = true)\n",
      " |-- processed_dttm: timestamp (nullable = true)\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- local_time: timestamp (nullable = true)\n",
      "\n",
      "+---------+----------+--------------------+-----------+--------------------+\n",
      "|user_left|user_right|      processed_dttm|    zone_id|          local_time|\n",
      "+---------+----------+--------------------+-----------+--------------------+\n",
      "|    94546|     22403|2022-12-02 19:30:...| Cranbourne|2022-12-03 06:30:...|\n",
      "|    79336|    163833|2022-12-02 19:30:...|  Newcastle|2022-12-03 06:30:...|\n",
      "|     8093|     77752|2022-12-02 19:30:...|  Newcastle|2022-12-03 06:30:...|\n",
      "|    91579|    108236|2022-12-02 19:30:...|  Newcastle|2022-12-03 06:30:...|\n",
      "|    94153|    146337|2022-12-02 19:30:...|  Newcastle|2022-12-03 06:30:...|\n",
      "|     1692|    118515|2022-12-02 19:30:...|Rockhampton|2022-12-03 05:30:...|\n",
      "+---------+----------+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .appName(\"Project 7\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "events = spark.read.parquet(\"/user/nickperegr/prj7/prod/data/analytics/recommendations/\")\n",
    "\n",
    "events.printSchema()\n",
    "\n",
    "#events.where(\"home_city is not null\").show(20, False)\n",
    "events.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c05ecbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "def get_date(date, days_count):\n",
    "\n",
    "    end_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    start_date = end_date - timedelta(days=days_count)\n",
    "    \n",
    "    print(datetime.strftime(start_date, '%Y-%m-%d'))\n",
    "    print(datetime.strftime(end_date, '%Y-%m-%d')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51417c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01\n",
      "2022-05-01\n"
     ]
    }
   ],
   "source": [
    "get_date(\"2022-05-01\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341ac5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
